# --- Core ---
API_KEY=changeme
PERSIST_DIR=/app/vector_store
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
READONLY_MODE=false

# --- Backends priority ---
MODEL_PRIORITY=ollama,openai

# --- OpenAI (optional cloud fallback/refine) ---
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# --- Ollama (local LLM) ---
# Scenario A: Ollama as docker-compose service (service name: ollama)
OLLAMA_HOST=http://ollama:11434
# Scenario B: Ollama on host machine (Windows/Mac) â€” uncomment below and comment the line above
# OLLAMA_HOST=http://host.docker.internal:11434

# Default local model (you can change to llama3, gpt-oss:20b, etc.)
OLLAMA_MODEL=llama3

DEFAULT_LANGUAGE=zh-tw
DOCS_DIR=/app/docs
KB_DB_PATH=/app/data/kb.sqlite
PERSIST_DIR=/app/vector_store
COLLECTION_NAME=gamefantasy

