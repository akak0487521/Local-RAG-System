x-ollama-common: &ollama_common
  container_name: ollama
  restart: unless-stopped
  ports:
    - "127.0.0.1:11435:11434"     # 外部用 11435 測試；容器間一律走 http://ollama:11434
  environment:
    - OLLAMA_HOST=0.0.0.0
    - OLLAMA_KEEP_ALIVE=24h
  volumes:
    - ~/.ollama:/root/.ollama        # 模型與快取持久化
    - ./modelfiles:/modelfiles:ro # 你的自訂 Modelfile（可選）
  healthcheck:
    test: ["CMD", "curl", "-fsS", "http://127.0.0.1:11434/api/tags"]
    interval: 30s
    timeout: 5s
    retries: 5
  networks:
    default:
      aliases: ["ollama"]         # 讓其他服務用 http://ollama:11434 連這台

services:
  ragapi:
    build: .
    container_name: ragapi
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./docs:/app/docs
      - ./vector_store:/app/vector_store
      - ./web:/app/web:ro
      - hf-cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://127.0.0.1:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 5
    # If your Ollama runs on the host instead of the compose service,
    # uncomment the following and set OLLAMA_HOST=http://host.docker.internal:11434 in .env
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"

  ollama-nvidia:
    <<: *ollama_common
    profiles: ["nvidia"]
    image: ollama/ollama:latest
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
  
  ollama-amd:
    <<: *ollama_common
    profiles: ["amd"]
    image: ollama/ollama:rocm
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video

    # For NVIDIA GPUs, uncomment (requires nvidia-container-toolkit on host)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: ["gpu"]
volumes:
  hf-cache: {}
  ollama: {}
